{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e44a34-f622-4626-a2d3-5b5987f61cae",
   "metadata": {},
   "source": [
    "## Unspoiled - AI Model Training\n",
    "#### Ben Cobb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06d782-28bf-4190-95e7-7cb76f16c88c",
   "metadata": {},
   "source": [
    "Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a896057-0f69-485e-a57c-1b33eba1d19f",
   "metadata": {},
   "source": [
    "## THINGS TO DO FIRST!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d176a2-c704-4b43-8a68-fef7f7641089",
   "metadata": {},
   "source": [
    "#### Uncomment and run the ones you need (Shift+Enter is how I run individual cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c8944c-de92-4f1d-9043-f37606db4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this if you don't already have YOLOv6 cloned on your system!\n",
    "# !git clone https://github.com/meituan/YOLOv6.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e72120-4dfd-4bb5-945c-21871d63defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When YOLOv6 is on your system, be sure to download data.zip from the GDrive link in our repo. \n",
    "# Replace the /YOLOv6/data folder with what's inside the zip!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26340707-8f64-4781-a3be-5581966e14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this if you haven't already installed Ultralytics!\n",
    "# !pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160f77de-3592-4d9c-b8c0-1bc3370878af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to change the yamlPath variable to match your system's path setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150ec513-aac6-4dc6-bca3-1b888b10bdf5",
   "metadata": {},
   "source": [
    "## Imports and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a3541-1257-4e9a-b6b0-f17e8a6a1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b86731d-8a5f-44fc-af42-bff4fc3e7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models.detection as detection\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import RoIAlign\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a80827-e4d3-4203-9f47-b2e2ec453391",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0a665-3ace-4c22-b5f2-706351c8a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the start of this path to the location your YOLOv6 folder is in!\n",
    "yamlPath = \"/Users/molte/YOLOv6/data/dataset.yaml\"\n",
    "\n",
    "# Change this when we have a real model! (for now, you can ignore)\n",
    "modelPath = \"/Users/molte/OneDrive/Desktop/UAFS/~Spring 2024/Capstone/Unspoiled/AIModel.yaml\"\n",
    "\n",
    "deptNames = [\"COLD\", \"PRODUCE\", \"PACKAGED\"]\n",
    "classes = [\"MILK\", \"EGGCARTON\", \"CREAMER\", \"APPLE\", \"BANANA\", \"PEAR\", \"COUGHDROPS\", \"CHEEZIT\", \"SODA\"]\n",
    "'''\n",
    "classIDs:\n",
    "0 = \"milk\"\n",
    "1 = \"eggcarton\"\n",
    "2 = \"creamer\"\n",
    "3 = \"apple\"\n",
    "4 = \"banana\"\n",
    "5 = \"pear\"\n",
    "6 = \"coughdrops\"\n",
    "7 = \"cheezit\"\n",
    "8 = \"soda\"\n",
    "'''\n",
    "\n",
    "cmap = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeba7ba-f966-4245-99ca-8ebb8372645d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1e41d03-08a4-4383-94d5-97a3e2667693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a photo\n",
    "def doStuff():\n",
    "\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66dbb3-0413-4f87-a83c-477c035664c7",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5889c40-0a04-422a-a575-e1b0f760bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for our Dataset\n",
    "class UnspoiledData(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, folder, transform=None):\n",
    "        self.filepath = filepath\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.imagePath = filepath + \"images/\" + folder\n",
    "        self.labelPath = filepath + \"labels/\" + folder\n",
    "\n",
    "        # Lists all image files in the images directory\n",
    "        self.image_files = [f for f in os.listdir(self.imagePath) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.imagePath, img_name)\n",
    "        annotation_path = os.path.join(self.labelPath, img_name.replace('.png', '.txt'))\n",
    "\n",
    "        # Reads image\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        # Reads bounding box annotations from text file\n",
    "        boxes = []\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                centerX, centerY, width, height = map(float, line.strip().split())\n",
    "                \n",
    "                # Converts centerX, centerY, width, height to x_min, y_min, x_max, y_max\n",
    "                x_min = (centerX - width / 2) * image.shape[2]\n",
    "                y_min = (centerY - height / 2) * image.shape[1]\n",
    "                x_max = (centerX + width / 2) * image.shape[2]\n",
    "                y_max = (centerY + height / 2) * image.shape[1]\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1758a25-40bb-4b4c-802d-095e06345197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the Object Detection and Classification Model\n",
    "class ObjectDetectionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ObjectDetectionClassifier, self).__init__()\n",
    "\n",
    "        # Defines the custom backbone network\n",
    "        # We can modify this section to define a different backbone architecture\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Defines the anchor generator for the Region Proposal Network (RPN)\n",
    "        anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256),),\n",
    "                                           aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "        # Defines the Region Proposal Network (RPN)\n",
    "        self.rpn = nn.Conv2d(256, anchor_generator.num_anchors_per_location()[0] * 4, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Defines the ROI Pooling module (using RoIAlign)\n",
    "        self.roi_pooler = RoIAlign(output_size=(7, 7), spatial_scale=1.0, sampling_ratio=2)\n",
    "\n",
    "        # Defines the final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 7 * 7, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        # Backbone feature extraction\n",
    "        features = self.backbone(images)\n",
    "\n",
    "        # Region Proposal Network (RPN)\n",
    "        proposals = self.rpn(features)\n",
    "\n",
    "        # ROI Pooling (RoIAlign)\n",
    "        box_features = self.roi_pooler(features, [proposals])\n",
    "\n",
    "        # Flattens features\n",
    "        box_features = box_features.view(box_features.size(0), -1)\n",
    "\n",
    "        # Classifier\n",
    "        class_logits = self.classifier(box_features)\n",
    "\n",
    "        if self.training:\n",
    "            return class_logits\n",
    "        else:\n",
    "            return class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333d7704-e5b1-4012-a454-f0667988e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnspoiledLoss(nn.Module):\n",
    "    def __init__(self, weight_bbox=1.0, weight_cls=1.0):\n",
    "        super(YourCustomLossFunction, self).__init__()\n",
    "        self.weight_bbox = weight_bbox\n",
    "        self.weight_cls = weight_cls\n",
    "        self.bbox_loss_fn = nn.SmoothL1Loss(reduction='mean')\n",
    "        self.cls_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, predicted_boxes, predicted_scores, target_boxes, target_labels):\n",
    "        \n",
    "        # Computes bounding box regression loss\n",
    "        bbox_loss = self.bbox_loss_fn(predicted_boxes, target_boxes)\n",
    "\n",
    "        # Computes classification loss\n",
    "        cls_loss = self.cls_loss_fn(predicted_scores, target_labels)\n",
    "\n",
    "        # Combines the losses (weighted sum)\n",
    "        total_loss = (self.weight_bbox * bbox_loss) + (self.weight_cls * cls_loss)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29e126-3ce4-4529-953b-9eecfdc84415",
   "metadata": {},
   "source": [
    "# Model Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c4e092b-a49e-404b-b73a-d30a31ce0e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectDetectionClassifier(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rpn): Conv2d(256, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (roi_pooler): RoIAlign(output_size=(7, 7), spatial_scale=1.0, sampling_ratio=2, aligned=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=1024, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defines the number of classes for our dataset\n",
    "num_classes = 9  # Adjust based on our number of classes\n",
    "\n",
    "# Instantiates our custom object detection and classification model\n",
    "model = ObjectDetectionClassifier(num_classes)\n",
    "\n",
    "# Loads pre-trained weights if available (optional)\n",
    "# Replace 'path_to_pretrained_weights.pth' with the path to your pretrained weights file\n",
    "# For example:\n",
    "# model.load_state_dict(torch.load('path_to_pretrained_weights.pth'))\n",
    "\n",
    "# Sets the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0bf47-cd29-4423-9ade-cb6132aa594d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857cc2fb-002b-4cff-aaac-353fcedf6870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a55b09-6c3a-49cb-8051-896d4545c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoI-Aligned Features Shape: torch.Size([2, 3, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# Example normalized bounding boxes (centerX, centerY, width, height)\n",
    "# We can replace these with our actual normalized bounding boxes\n",
    "boxes_normalized = torch.tensor([[0.5, 0.5, 0.4, 0.3], [0.3, 0.4, 0.2, 0.5]])  # Example: Two normalized bounding boxes\n",
    "\n",
    "# Converts normalized bounding boxes to absolute coordinates (x_min, y_min, x_max, y_max)\n",
    "# centerX = boxes_normalized[:, 0], centerY = boxes_normalized[:, 1]\n",
    "# width = boxes_normalized[:, 2], height = boxes_normalized[:, 3]\n",
    "xCenter = boxes_normalized[:, 0] * 320\n",
    "yCenter = boxes_normalized[:, 1] * 320\n",
    "width = boxes_normalized[:, 2] * 320\n",
    "height = boxes_normalized[:, 3] * 320\n",
    "\n",
    "xMin = xCenter - (width / 2)\n",
    "yMin = yCenter - (height / 2)\n",
    "xMax = xCenter + (width / 2)\n",
    "yMax = yCenter + (height / 2)\n",
    "\n",
    "# Combines into bounding boxes tensor [N, 4] (x_min, y_min, x_max, y_max)\n",
    "boxes_abs = torch.stack([x_min, y_min, x_max, y_max], dim=1)\n",
    "\n",
    "# Ensure that the boxes tensor has the correct shape [N, 4]\n",
    "assert boxes_abs.dim() == 2 and boxes_abs.size(1) == 4, \"Boxes tensor should have shape [N, 4]\"\n",
    "\n",
    "# Convert the bounding box tensor to a list of tensors (for RoIAlign)\n",
    "boxes_list = [boxes_abs]\n",
    "\n",
    "# Example input tensor (batch size = 1, 3 channels, height = 320, width = 320)\n",
    "images = torch.randn(1, 3, 320, 320)  # Assuming 320x320 RGB image\n",
    "\n",
    "# Initialize RoIAlign module\n",
    "roi_align = RoIAlign(output_size=(7, 7), spatial_scale=1.0, sampling_ratio=2)\n",
    "\n",
    "# Apply RoIAlign with the prepared boxes list on the input images\n",
    "with torch.no_grad():\n",
    "    roi_features = roi_align(images, boxes_list)\n",
    "\n",
    "# Print the shape of the RoI-aligned features\n",
    "print(\"RoI-Aligned Features Shape:\", roi_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d423128-1ede-42a1-9973-82bc66403d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08c1b1f8-c129-4043-9bf8-17a23e1f241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# train/test/val\n",
    "folder = \"train\"\n",
    "filepath = \"/Users/molte/OneDrive/Desktop/UAFS/~Spring 2024/Capstone/Unspoiled/AIModelTraining/dataset/\"\n",
    "\n",
    "# Create instances of your custom dataset (train and validation sets)\n",
    "trainData = UnspoiledData(filepath, \"train\", transform=None)\n",
    "valData = UnspoiledData(filepath, \"val\", transform=None)\n",
    "\n",
    "\n",
    "batchSize = 3\n",
    "numWorkers = 2\n",
    "\n",
    "# Create data loaders\n",
    "trainLoader = DataLoader(trainData, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "valLoader = DataLoader(valData, batch_size=batchSize, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "print(\"data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9056c157-8abb-4509-843e-c1d8f3315649",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YourCustomLossFunction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define loss function (example)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m \u001b[43mYourCustomLossFunction\u001b[49m()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Define optimizer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'YourCustomLossFunction' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function\n",
    "criterion = UnspoiledLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fbedb-43a5-4d3f-b927-534d814e96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "print(\"before device\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"before model.to\")\n",
    "model.to(device)\n",
    "print(\"before loop\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"train started\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(\"train done\")\n",
    "    for images, targets in trainLoader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, targets)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        print(\"Loss:\", loss)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print training loss for each epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "    # Validation (optional)\n",
    "    model.eval()\n",
    "    # Perform validation if needed\n",
    "\n",
    "# Save trained model weights\n",
    "torch.save(model.state_dict(), 'path_to_save_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c83f0-6b4e-4e95-922e-c6c94d1393a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24da93-ee12-4ad0-b03f-8e0df405a0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1140b-3bf5-42e7-bcee-3584a8f83f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a08f4-be28-4c5d-b046-e4246734e608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46b824bb-9e54-43d8-b8fd-2a8230b17bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    boxes = result.boxes\n",
    "    masks = result.masks\n",
    "    keypoints = result.keypoints\n",
    "    probs = result.probs\n",
    "    \n",
    "    imgArray = result.plot()  # plot a BGR numpy array of predictions\n",
    "    img = Image.fromarray(imgArray[..., ::-1])  # RGB PIL image\n",
    "    img.show()  # show image\n",
    "    img.save('./images_OLD/results.jpg')  # save image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b60e6-454f-4bf6-bfa9-7e7eea52d368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
